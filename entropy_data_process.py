import matplotlib.pyplot as plt
import matplotlib
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier
import matplotlib.dates as mdates
from mplfinance.original_flavor import candlestick_ohlc
from tqdm import tqdm

from entropy import SampleEntropy

matplotlib.use('TkAgg')


class EntropyDataProcess:
    def __init__(self, file_path):
        self.file_path = file_path
        self.se = SampleEntropy()

    def to_min_of(self, minute):
        import pandas as pd

        df = pd.read_csv(self.file_path, parse_dates=['date'])
        df.set_index('date', inplace=True)

        # é‡é‡‡æ ·ä¸º3åˆ†é’Ÿæ•°æ®
        df_min = df.resample(f'{minute}T').agg({
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        }).dropna().reset_index()
        df_min.to_csv(f"LC_20230721_20251030_{minute}min.csv", index=False)
        return df_min

    def calc_entropy_avg_as_threshold(self):
        df = pd.read_csv(self.file_path, parse_dates=['date'])
        if 'close' not in df.columns:
            raise ValueError("DataFrame must contain 'close' column")

        # è®¡ç®—æ”¶ç›Šç‡
        df['return'] = df['close'].pct_change().shift(-1)
        df = df.dropna().reset_index(drop=True)

        # æ¯4åˆ†é’Ÿåˆ†ç»„è®¡ç®—ç†µ
        window_size = 4
        entropy_list = []
        for i in tqdm(range(0, len(df), window_size), desc="Calculating Entropy"):
            window = df['return'].iloc[i:i + window_size].values
            if len(window) < window_size:
                continue
            H = SampleEntropy().renyi_entropy(window, sigma=2, alpha=0.01)
            entropy_list.append(H)

        # è®¡ç®—å¹³å‡ç†µ
        avg_entropy = np.nanmean(entropy_list)
        return avg_entropy

    def resample_to_entropy(self):
        """æŒ‰ç†µé˜ˆå€¼ç”Ÿæˆç­‰ç†µ bar"""
        threshold = self.calc_entropy_avg_as_threshold()

        df = pd.read_csv(self.file_path, parse_dates=['date'])
        df['return'] = df['close'].pct_change().shift(-1)
        # å¯¹close å»é‡çº²
        df = df.dropna().reset_index(drop=True)

        bars = []
        temp_window = []
        for i, r in enumerate(df['return']):
            temp_window.append(r)
            window_array = np.array(temp_window)
            H = self.se.renyi_entropy(window_array, sigma=2, alpha=0.01)
            if H >= threshold:
                print(i)
                # è¾¾åˆ°é˜ˆå€¼ç”Ÿæˆä¸€æ ¹ bar
                bar = {
                    'start_index': i - len(temp_window) + 1,
                    'end_index': i,
                    'open': df['close'].iloc[i - len(temp_window) + 1],
                    'close': df['close'].iloc[i],
                    'high': df['close'].iloc[i - len(temp_window) + 1:i + 1].max(),
                    'low': df['close'].iloc[i - len(temp_window) + 1:i + 1].min(),
                    'volume': df['volume'].iloc[i - len(temp_window) + 1:i + 1].sum(),
                    'entropy': H,
                    'count': len(temp_window)
                }
                bars.append(bar)
                temp_window = []  # æ¸…ç©ºçª—å£ï¼Œé‡æ–°ç´¯ç§¯

        bars_df = pd.DataFrame(bars)
        bars_df.to_csv("LC_20230721_20251030_entropybar.csv", index=False)
        return bars_df

    def get_pct(self):
        df_volume = pd.read_csv("LC_20230721_20251030_entropybar.csv")
        # å½“å‰è¡Œä¸ä¸‹ä¸€è¡Œçš„å˜åŒ–
        df_volume['pct_change_1'] = df_volume['close'].pct_change(periods=1).shift(-1)

        # å½“å‰è¡Œä¸åä¸¤è¡Œçš„å˜åŒ–
        df_volume['pct_change_2'] = (df_volume['close'].shift(-2) - df_volume['close']) / df_volume['close']
        df_volume['pct_change_5'] = (df_volume['close'].shift(-5) - df_volume['close']) / df_volume['close']
        return df_volume

    def plot_volume_over_time(self, minute):
        df = self.to_min_of(minute)
        self.plot_data_distribution(minute, df, 'volume')

    def plot_data_distribution(self, minute, df, label):
        # è®¡ç®—ä¸­ä½æ•°
        print(f"Plotting distribution for {minute}-Minute {label}")
        V_bar = df[label].quantile(0.5)
        print(f"Median volume (50% quantile) as threshold: {V_bar}")

        # è®¡ç®—å¹³å‡å€¼
        mean_value = df[label].mean()
        print(f"Mean volume: {mean_value}")

        plt.figure(figsize=(10, 5))
        plt.hist(
            df[label],
            bins=100,
            edgecolor='black',
            density=True  # <== yè½´å½’ä¸€åŒ–åˆ°æ¦‚ç‡å¯†åº¦
        )
        plt.title(f'{minute}-Minute {label} Distribution')
        plt.xlabel(f'{label}')
        plt.ylabel('Density (0-1)')
        # ç»˜åˆ¶ä¸­ä½æ•°çº¿
        plt.axvline(V_bar, color='red', linestyle='--', label=f'Median={V_bar:.2f}')
        plt.legend()
        # ç»˜åˆ¶å¹³å‡å€¼çº¿
        plt.axvline(mean_value, color='green', linestyle='--', label=f'Mean={mean_value:.2f}')
        plt.legend()
        # plt.show()
        plt.savefig(f'{minute}min_{label}_distribution.png')

    def plot_data(self):
        df = pd.read_csv(self.file_path, parse_dates=['date'])
        plt.figure(figsize=(12, 6))
        plt.plot(df['date'], df['volume'], label='3-Minute Volume')
        plt.xlabel('Date')
        plt.ylabel('Volume')
        plt.title('3-Minute Resampled Volume Data')
        plt.legend()
        plt.show()

    def get_threshold(self):
        pct = self.get_pct()
        label = pct['pct_change_1'].dropna()  # å»æ‰ NaN

        # å³ä¾§ç´¯ç§¯æ¦‚ç‡ 0.35 å¯¹åº”å·¦ä¾§ç´¯ç§¯æ¦‚ç‡ 0.65
        threshold = np.quantile(label, 0.6)

        print("å³ä¾§é¢ç§¯0.35å¯¹åº”çš„é˜ˆå€¼:", threshold)
        # 0.0002714
        return threshold

    def mark_label(self):
        df_volume = pd.read_csv("LC_20230721_20251030_entropybar.csv")
        df_volume['pct_change_1'] = df_volume['close'].pct_change(periods=1).shift(-1)
        threshold = self.get_threshold()
        df_volume['label'] = (df_volume['pct_change_1'] > threshold).astype(int)
        return df_volume

    def train_model_RF(self):
        df_3min = self.mark_label()

        # ====== é€‰æ‹©ç‰¹å¾åˆ— ======
        # æ ¹æ®ä½ çš„æ•°æ®ç»“æ„è‡ªè¡Œä¿®æ”¹ï¼Œå¦‚æœä½ æ²¡æœ‰å…¶å®ƒç‰¹å¾ï¼Œå¯å…ˆç”¨ price ç±»ç‰¹å¾æµ‹è¯•
        feature_cols = [
            'open', 'high', 'low', 'close', 'volume'
        ]
        feature_cols = [c for c in feature_cols if c in df_3min.columns]

        if not feature_cols:
            raise ValueError("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨ç‰¹å¾åˆ—ï¼Œè¯·æ£€æŸ¥ CSV ä¸­æ˜¯å¦åŒ…å« open/high/low/close/volume")

        X = df_3min[feature_cols]
        y = df_3min['label']

        n_samples = len(X)
        train_size = int(n_samples * 0.7)  # 70% è®­ç»ƒé›†ï¼Œ30% æµ‹è¯•é›†

        # æ‰‹åŠ¨åˆ’åˆ†
        X_train = X[:train_size]
        X_test = X[train_size:]
        y_train = y[:train_size]
        y_test = y[train_size:]

        # # ====== åˆ’åˆ†è®­ç»ƒ / æµ‹è¯• ======
        # X_train, X_test, y_train, y_test = train_test_split(
        #     X, y, test_size=0.3, shuffle=False  # é¢„æµ‹æ—¶é—´åºåˆ—ä¸æ‰“ä¹±
        # )
        # ç”¨Z score å»é‡çº²

        X_train['close_zscore'] = (X_train['close'] - X_train['close'].mean()) / X_train['close'].std()
        X_train['open_zscore'] = (X_train['open'] - X_train['open'].mean()) / X_train['open'].std()
        X_train['high_zscore'] = (X_train['high'] - X_train['high'].mean()) / X_train['high'].std()
        X_train['low_zscore'] = (X_train['low'] - X_train['low'].mean()) / X_train['low'].std()
        X_train['volume_zscore'] = (X_train['volume'] - X_train['volume'].mean()) / X_train['volume'].std()
        X_test['close_zscore'] = (X_test['close'] - X_train['close'].mean()) / X_train['close'].std()
        X_test['open_zscore'] = (X_test['open'] - X_train['open'].mean()) / X_train['open'].std()
        X_test['high_zscore'] = (X_test['high'] - X_train['high'].mean()) / X_train['high'].std()
        X_test['low_zscore'] = (X_test['low'] - X_train['low'].mean()) / X_train['low'].std()
        X_test['volume_zscore'] = (X_test['volume'] - X_train['volume'].mean()) / X_train['volume'].std()

        XTrain = X_train[['close_zscore', 'open_zscore', 'high_zscore', 'low_zscore', 'volume_zscore']]
        XTest = X_test[['close_zscore', 'open_zscore', 'high_zscore', 'low_zscore', 'volume_zscore']]

        # ====== éšæœºæ£®æ— ======
        model = RandomForestClassifier(
            n_estimators=300,
            max_depth=15,
            min_samples_split=10,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        )

        # model = RandomForestClassifier(
        #     n_estimators=100,
        #     criterion='gini',
        #     random_state=42,
        #     class_weight='balanced'
        # )

        model.fit(XTrain, y_train)

        ####
        y_train_pred = model.predict(XTrain)
        print("ğŸ¯ RandomForest Train Accuracy:", accuracy_score(y_train, y_train_pred))
        print(classification_report(y_train, y_train_pred))
        # ====== é¢„æµ‹ ======
        y_pred = model.predict(XTest)

        # ====== è¾“å‡ºç»“æœ ======
        print("ğŸ¯ RandomForest Accuracy:", accuracy_score(y_test, y_pred))
        print("\nğŸ“‹ Classification Report:")
        print(classification_report(y_test, y_pred))
        df_con = pd.DataFrame({
            'Actual': y_test,
            'Predicted': y_pred
        })

        # ====== ç‰¹å¾é‡è¦æ€§ ======
        fi = pd.DataFrame({
            'feature': feature_cols,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        print("\nğŸ” Feature Importance:")
        print(fi)

        return model, fi


if __name__ == '__main__':
    data_process = EntropyDataProcess("LC_20230731_20251030.csv")
    df = data_process.to_min_of()
    print(df.head(5))
